## Мотивация
Создание новых супер крутых эмбеддингов для мультиязычных задач, используя лучшие современные метода, такие как [[BERT]] и его варианты.
## Архитектура
![[Pasted image 20241009200051.png|400]]
Используется два энкодера из BERT, один из них получает эмбеддинг текста на исходном языке, второй же на целевом языке. После чего от этих эмбеддингов вычисляется следующий лосс:  

$\Huge \mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{\phi(x_i, y_i)}}{e^{\phi(x_i, y_i)} + \sum_{n=1, n \neq i}^{N} e^{\phi(x_i, y_n)}}$

Здесь $\phi(x_i, y_i)$ функция близости, обычно это dot-product $\phi(x, y) = xy^T$. Используется batch negative sampling, то есть в батче один правильный пример, и N-1 negative примеров, которые выбираются случайным образом. Таким образом, чем ближе эмбеддинги одного и того же текста на разных языках, и чем дальше негативные примеры, тем более будет дробь под $\log$, что нам и нужно так как перед ней стоит минус. Можно заметить, что эта лосс функция несиммитрична, относительно source и target языков, то есть мы берем только source-target перевод, для симметричности добавляется второе слагаемое, которое является тем же самым, только в обратном направлении. 
Итого лосс выглядит следующим образом:
$\Huge \bar{\mathcal{L}} = \mathcal{L'} + \mathcal{L}$

Также добавляется отступ $m$, благодаря которому получается еще лучше разделять эмбеддинги. 

$\huge \phi'(x_i, y_j) = 
\begin{cases} 
\phi(x_i, y_j) - m & \text{если } i = j \\ 
\phi(x_i, y_j) & \text{если } i \neq j 
\end{cases}
\tag{3}$

Из формулы видно что отступ вынуждает быть значение функции $\phi$ быть еще больше.



$\Huge \mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{\phi(x_i, y_i) - m}}{e^{\phi(x_i, y_i) - m} + \sum_{n=1, n \neq i}^{N} e^{\phi(x_i, y_n)}}\tag{4}$


## Cross-Accelerator Negative Sampling

Для лучшего разделения примеров используется Negative Sampling. Чем больше в нем будет негативных примеров, тем лучше, но упираемся в ограничение по ресурсам на одно ядро. Cross-Accelerator Negative Sampling позволяет обмениваться отрицательными примерами между различными устройствами. Вместо того чтобы ограничиваться отрицательными примерами на одном GPU, модель использует отрицательные примеры с других GPU, увеличивая разнообразие данных.
![[Pasted image 20241010234209.png]]