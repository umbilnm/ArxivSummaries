Фокус и идея этой работы в том, чтобы 
- обучить модель только на публичных датасетах, то есть сделать OpenSource модель которая сможет соревноваться с лучшими моделями того временного промежутка(2023 г.)
- обучить серию моделей, которые будут лучшими в своем inference budget
- обучать модели на большем кол-ве токенов, чем это делалось обычно
![[Pasted image 20241012214348.png|450]]
**Данные для обучения** 
![[Pasted image 20241012211622.png|450]]

**Конфигурации моделей:**
![[Pasted image 20241012211849.png|500]]

## Архитектура
- В отличие от оригинальной статьи [[Attention is all you need..]], где Add&Norm слой стоит **после** каждого MultiHeadAttention и FeedForward, слой нормализации стоит **перед** каждым из этих слоев, используют [[RMSNorm and LayerNorm]]
- Вместо ReLU в качестве функции активации используется **SwishGLU**
- Вместо абсолютных позиционных эмбеддингов используются [[RoPE]]
- Используется оптимизированный attention из xformers, который не рассчитывает attention scores для токенов, которые позднее будут маскированы
