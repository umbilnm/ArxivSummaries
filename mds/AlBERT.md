Мотивация: После выхода [[BERT]] в 2018 году, который получился хорошей моделью для своих задач, но из-за его размеров (340 миллионов параметров). Что препятствовало его развертыванию, процессу обучения и масштабируемости. Поэтому ребята из Гугла захотели решить эти проблемы.

## Архитектура

### Factorizered embedding parameterization
Как это было в BERT, да и вроде вообще всегда: у нас есть Vocab размером $VocabSize*EmbedDim$, то есть из one-hot вектора токена мы получаем его эмбеддинг. Причем в оригинанальном BERT использовались вектора WordPiece(не помню про них, позднее дополню, НО! они были обучены независимо от контекста). И при этом размерность скрытого слоя $H$ совпадала с размерностью эмбеддинга WordPiece, что не до конца эффективно,  так как мы хотим получить эмбеддинги, которые отражали бы контекст, то интуитивно понятно что у этих эмбеддингов размерность должна быть больше чем у эмбеддингов без контекста(WordPiece). Можно было бы просто увеличить размерность эмбеддинга, но тогда наша матрица, размеры которой $VocabSize * EmbedDim$ росла бы очень быстро, что некруто, так как ващет хотели оптимизировать скорость/потребление памяти.

Итого: в [[BERT]] размерность словаря равна 30000, размерность эмббеддинга 768. Размер матрицы: 30000 x 768. Авторы AlBERT оптимизировали это следующим хаком: разбили матрицу размеров $V*H$, при том $H=E=768$ в BERT-Base, на две матрицы размерами $V * E$ и $E*H$. Что при $E=128$ и $H=768$ уменьшило кол-во параметров на ~**40%**

### Cross-Layer Parameter Sharing
В каждом слое трансформера используются одни и те же матрицы $Q, K, V$, а также матрицы $W$ в feed-forward слое, что очень сильно экономит размеры модели. Например, в ALBERT Large, при 24 слоях, было бы 24 уникальных набора параметров для self-attention и feed-forward сетей, но с  используется только один набор. Причем такая модель все еще показывает результаты лучше [[BERT]] на многих задачах  
![[Pasted image 20241009155302.png]]
Также было замечено следующее: расстояния между выходными эмбеддингами на различных слоях модели **гораздо** более стабильны, если сравнивать с [[BERT]]. Что говорит о том, что такой прием помогает сделать обучение нейросети более стабильным.
![[Pasted image 20241009160659.png]]
В BERT $L2$ и косинусное расстояния между эмбеддингами на разных слоях даже после 24 слоя все еще очень нестабильны.
##  Задачи обучения
Также в процессе обучения было решено решать задачу SOP(Sentence Order Prediction) вместо NSP(Next Sentence Prediction). Дается два предложения и надо понять какое у них взаимное расположение. Т.е. предложение 2 идет после предложения 1 или наоборот.

Если для решения задачи NSP требовалось лишь верхнеуровнево понять связаны ли два предложения, то для решения задачи SOP требуется улавливать связь внутри текста. Таким образом, SOP является более сложной задачей, так как требует от модели не только понимания связи между сегментами, но и их правильного порядка, что может быть более полезно для задач, где важна логическая структура текста.

P.S. задача MLM никуда не ушла, она так же решалась



