Мотивация: с выхода [[BERT]] в 2018 прошло очень много времени и появилось множество инженерных улучшений, в основном области LLM(decoder only моделей), в то время как в мире encoder-only моделей давно уже не было улучшешений без tradeoff'ов. ![[modernbert_pareto_curve.png|500]]

Идея авторов достаточно просто: взять все инженерные новшества, появившиеся в последнее время, и применить к encoder-only модели.

Среди них:
- модернизированная архитектура трансформера
- новые attention механики
- современные источники данных


## Архитектурные улучшения

- авторы убрали bias из линейных слоев
- используют RoPE в качестве позиционных эмбеддингов
- используется pre-norm
### Alternating Attention

Идея в том, чтобы сократить время рассчитывания atttention слоев, поэтому авторы используют классический attention,  где каждый токен "смотрит" на каждый раз в три слоя. В остальных случаях используется sliding window attention с окном в 128 токенов.


### Unpadding and sequence packing

![[modernbert_unpadding.png]] 

При обычном паддинге compute тратится на токены, которые не несут никакого смысла(PAD токены), при Sequence Packing все элементы батча конкатинируются таким образом, что получается батч из одного элемента(Sequence Packing).

## Обучение

1. Учат на задаче MLM(Masked Language Modeling), при этом убрав задачу NSP. 
2.  Токенайзер от модели OLMo
3. Vocab состоит из 50368 токенов, 83 из которых не используются(50368 = 64 * 787, эффективнее)
4. Маскируют 30% токенов в отличие от 15% у оригинального BERT'а
5. Веса large модели в начале обучения инициализируют весами base модели
6.  Сначала обучают на 1.7 триллиона токенов с длиной последовательности в 1024 токена, после чего еще на 300 миллионах токенов с длиной последовательности в 8192 токена, что позволяет добиться длины контекста в 8192 токенов(512 у BERT)
