Формулы для LayerNorm
$$
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta
$$

где:

$$
\mu = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

$$
\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
$$

- $\gamma$  и $\beta$: обучаемые параметры для масштабирования и смещения.

**Формулы для RMSNorm:**
![[Pasted image 20241012230812.png]]


В формуле видно, что в отличие от $LayerNorm$ никак не учитывается среднее, из-за чего отсутствует инвариантность к сдвигу. Но авторы это понимают, более того, они считают что это не главная причина успеха LayerNorm'а, поэтому и убирают вычитание среднего. Благодаря этому нормализация становится быстрее. 
