# Text-to-Text Transfer Transformer
Идея работы в том, чтобы представить любую NLP задачу как text-to-text задачу, в т.ч. задачи классификации, суммаризации, что позволяет использовать одну и ту же модель, функцию потерь, процесс обучения для различных моделей.
![[Pasted image 20241027165252.png|650]]

Используются Transformer, в котором
- PreNorm
- позиционные эмбеддинги RoPE
-  $d_{ff}=3072$
- $d_{model}=768$
- Нелинейность - ReLU

Метрики замеряли на следующих задачах:
- машинный перевод
- QA
- суммаризация
- классификация

## Обучение 
Каждая задача была сформулирована в формате text-to-text, поэтому для каждой задачи был свой прификс. Например при переводе с английского языка на немецкий входящая строка может выглядеть так: *“translate English to German: That is good. target:”* ожидается что модель предскажет строку  *"Das ist gut"*

# Префиксное маскирование
Какое внимание использовать? Маскированное нельзя так как, например, потеряем информация английского предложения. А если сделать без маски, то получится так, что "заглядываем" в будущее при генерации ответа. Авторы предлагают использовать префиксное маскирование, идея которого заключается в следующем: в примере выше при обучении будем использовать attention без маскирования для входящей последовательности, то есть  *“translate English to German: That is good. target:”*, но для target последовательности используем masked attention. 

![[Pasted image 20241027182540.png| 200]]

Такой подход может напоминать BERT, так как мы используем внимание без маски на входящей последовательности, а потом делаем на ней предсказание, но их отличие в том что classifier в случае T5 уже встроен в архитектуру декодера.

## Objectives
![[Pasted image 20241027184846.png]]

Авторы рассмотрели несколько способов максирования текста в качестве задачи для обучения. Начали с первых трех, по бенчмарку получилось, что BERT-style objective дает лучшие результаты, после чего проверили другие варианты, основанные на нем:
- MASS-objective, BERT-style, но случайных токенов при маскировании, всегда используется токен \<M>
- drop маскированных токенов
- объединение нескольких последовательных маскированных токенов в один
![[Pasted image 20241027192015.png| 500]]